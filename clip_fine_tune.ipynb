{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59652ed5-5df9-4579-95f7-79039d4510cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/gery/clip/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3f9facb-c5a6-438b-9c7c-c9d6c783d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (\"data/app-launch.png\", \"Illustrasi popsy\"),\n",
    "    (\"data/shaking-hands.png\", \"Ilustrasi popsy\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ad63981-2577-4e33-919d-f9412c978ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset \n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text = self.dataset[idx]  # Ambil path gambar dan teks\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Pastikan gambar dalam format RGB\n",
    "        inputs = self.processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d87eaee-0c96-445c-873a-c346ae80f229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# SETUP TRAINING\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# load data\n",
    "train_dataset = CustomDataset(dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4a50afe-efdb-43af-8bb4-659c46c6031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mapp-launch.png\u001b[0m*  \u001b[01;32mshaking-hands.png\u001b[0m*  \u001b[01;32mwork-party.png\u001b[0m*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f02b6ce-37b7-427a-b178-79dd11b8522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def process_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    return inputs\n",
    "\n",
    "image = \"data/app-launch.png\"\n",
    "inputs = process_image(image)\n",
    "print(inputs['pixel_values'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d887ad6-ad19-458f-b87c-1033bc46740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9376, grad_fn=<DivBackward0>)\n",
      "Epoch 1, Loss: 0.9376348853111267\n",
      "tensor(1.2747, grad_fn=<DivBackward0>)\n",
      "Epoch 2, Loss: 1.2746667861938477\n",
      "tensor(0.1322, grad_fn=<DivBackward0>)\n",
      "Epoch 3, Loss: 0.13217653334140778\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    for batch in train_dataloader:\n",
    "        pixel_values = batch['pixel_values'].squeeze(1)  # Mengambil nilai pixel dari gambar\n",
    "        input_ids = batch['input_ids'].squeeze(1)  # Mengambil input_ids dari teks\n",
    "\n",
    "        pixel_values = pixel_values.to(model.device)\n",
    "        input_ids = input_ids.to(model.device)\n",
    "\n",
    "        # Memastikan return_loss=True\n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, return_loss=True)\n",
    "\n",
    "        # Periksa apakah 'loss' ada dalam output\n",
    "        if outputs.loss is not None:\n",
    "            print(outputs.loss)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "        else:\n",
    "            print(f\"No loss returned for this batch in Epoch {epoch + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a71dafb-e820-4b54-b55d-0a794d1d75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c5690e4-da76-48d4-89d4-4f055bedf24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f73a47bd-0e33-48d9-a809-1b3af9723b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"saved_model\")\n",
    "processor = CLIPProcessor.from_pretrained(\"saved_model\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81f0ff3a-fc6c-4f9a-a567-3c4cd79a59ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi probabilitas untuk gambar terhadap teks: tensor([[0.8071, 0.1929]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"saved_model\")\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "image_path = \"data/engineer.png\"\n",
    "text = [\"Illustration popsy\", \"person\"]\n",
    "\n",
    "image = Image.open(image_path)\n",
    "\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad(): \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image \n",
    "logits_per_text = outputs.logits_per_text \n",
    "\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "print(f\"Prediksi probabilitas untuk gambar terhadap teks: {probs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8581c5-f791-476f-a9bc-1428d15952a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
